# Test Crawler Locally - Quick Guide

## âœ… ÄÃ£ test thÃ nh cÃ´ng

Crawler Ä‘Ã£ Ä‘Æ°á»£c test vÃ  hoáº¡t Ä‘á»™ng tá»‘t vá»›i luatsubaoho.com

### Káº¿t quáº£ test

- **Downloaded**: 5 files (4 .doc, 1 .jpg)
- **CSV generated**: âœ…
- **Logs**: âœ…
- **2-level crawling**: âœ…
- **Date filtering**: âœ…
- **Keyword matching**: âœ…

## ğŸš€ CÃ¡ch test trÃªn local

### BÆ°á»›c 1: Activate virtual environment

```bash
source .venv/bin/activate
```

### BÆ°á»›c 2: Install dependencies (chá»‰ cáº§n 1 láº§n)

```bash
pip install -r requirements-crawler.txt
```

### BÆ°á»›c 3: Run test

```bash
# Option 1: Quick test vá»›i single URL
python test_crawler_local.py

# Option 2: Test vá»›i custom URLs
CRAWLER_TARGETS="https://your-url.com" \
DB_DATE="2020-01-01" \
python src/vietnamese_form_crawler.py

# Option 3: DÃ¹ng Makefile
make crawler-test
```

### BÆ°á»›c 4: Xem káº¿t quáº£

```bash
# View CSV
cat crawler_output/downloaded_files.csv

# List downloaded files
ls -lh crawler_output/*.{pdf,doc,docx,xlsx}

# View logs
tail -f crawler_output/crawler.log

# Hoáº·c dÃ¹ng Makefile
make crawler-results
```

## ğŸ“Š Output máº«u

### CSV Format

```csv
Tieu_de_trang,Link_file,Ten_file,Dang_tep,Ngay_dang
"Máº«u Ä‘Æ¡n xin viá»‡c",https://...,mau-don.doc,.doc,2025-11-05
```

### Files Downloaded

```
crawler_output/
â”œâ”€â”€ mau-don-phan-to.docx (24K)
â”œâ”€â”€ mau-giay-cam-ket.doc (35K)
â”œâ”€â”€ mau-to-khai-khai-tu.doc (44K)
â””â”€â”€ downloaded_files.csv (1.5K)
```

## ğŸ› ï¸ Makefile Commands

```bash
make crawler-install   # Install dependencies
make crawler-test      # Quick test
make crawler-run       # Full run
make crawler-results   # View results
make crawler-clean     # Clean output
```

## ğŸ¯ Test URLs Ä‘Ã£ thá»­

### âœ… Hoáº¡t Ä‘á»™ng tá»‘t

- `https://luatsubaoho.com/phapluat/mau-don-dang-ky-bien-dong-dat-dai-co-huong-dan-cach-viet/`
- Crawl Ä‘Æ°á»£c 5 files (.doc, .docx, .jpg)
- Date parsing OK
- 2-level crawling OK

### âŒ Bá»‹ cháº·n (403 Forbidden)

- `https://thuvienphapluat.vn/*` - Cloudflare protection máº¡nh

### ğŸ’¡ Gá»£i Ã½

Má»™t sá»‘ websites cÃ³ anti-bot máº¡nh (Cloudflare, reCAPTCHA). Náº¿u gáº·p lá»—i 403:

1. Thá»­ URL khÃ¡c
2. TÄƒng delay: `DELAY_BETWEEN_REQUESTS=3.0`
3. DÃ¹ng Selenium (cháº­m hÆ¡n nhÆ°ng bypass Ä‘Æ°á»£c JS)

## ğŸ”§ Troubleshooting

### Lá»—i: "No files downloaded"

```bash
# Giáº£m date cutoff
DB_DATE=2020-01-01 python test_crawler_local.py
```

### Lá»—i: "403 Forbidden"

```bash
# Test vá»›i URL khÃ¡c
# Hoáº·c tÄƒng delay
DELAY_BETWEEN_REQUESTS=5.0 python src/vietnamese_form_crawler.py
```

### Lá»—i: "Import bs4 not found"

```bash
source .venv/bin/activate
pip install -r requirements-crawler.txt
```

## âœ¨ Next Steps

Sau khi test OK trÃªn local:

1. Commit code: `git add . && git commit -m "Test crawler OK"`
2. Push: `git push origin main`
3. GitHub Action sáº½ tá»± Ä‘á»™ng cháº¡y daily lÃºc 2AM UTC

Hoáº·c trigger manual:

- VÃ o GitHub Actions tab
- Chá»n "Daily Crawler"
- Click "Run workflow"

## ğŸ“ Notes

- Virtual env: `.venv/` (Ä‘Ã£ cÃ³ sáºµn trong project)
- Output: `crawler_output/`
- Config: `.env` hoáº·c environment variables
- Logs: `crawler_output/crawler.log`
