# Form Processing Quick Start

## üöÄ Ph∆∞∆°ng √°n 1.5 ƒë√£ ƒë∆∞·ª£c implement

Pipeline t·ª± ƒë·ªông ƒë·ªÉ x·ª≠ l√Ω forms t·ª´ crawler th√†nh c·∫•u tr√∫c JSON t∆∞∆°ng th√≠ch v·ªõi h·ªá th·ªëng.

## Ki·∫øn tr√∫c

```
Crawler ‚Üí Form Processor ‚Üí Form Merger ‚Üí Form Search ‚Üí API
```

## C√†i ƒë·∫∑t nhanh

```bash
# 1. Install dependencies
pip install -r requirements.txt
pip install -r requirements-crawler.txt

# 2. Crawl forms (n·∫øu ch∆∞a c√≥)
make crawler-run

# 3. X·ª≠ l√Ω v√† merge forms
make forms-pipeline

# 4. T√¨m ki·∫øm forms
make forms-search Q="ƒë∆°n xin vi·ªác"
```

## Commands quan tr·ªçng

| Command | M√¥ t·∫£ |
|---------|-------|
| `make forms-process` | X·ª≠ l√Ω files t·ª´ crawler ‚Üí JSON |
| `make forms-merge` | Merge manual + crawled forms |
| `make forms-search Q="..."` | T√¨m ki·∫øm forms |
| `make forms-list` | List t·∫•t c·∫£ forms |
| `make forms-pipeline` | Ch·∫°y full pipeline |

## C·∫•u tr√∫c th∆∞ m·ª•c

```
forms/
‚îú‚îÄ‚îÄ form_samples.json          # Forms th·ªß c√¥ng (5 forms)
‚îú‚îÄ‚îÄ crawled_forms/             # Forms t·ª´ crawler
‚îÇ   ‚îú‚îÄ‚îÄ don_phan_to.json       # Example: ƒê∆°n ph·∫£n t·ªë
‚îÇ   ‚îî‚îÄ‚îÄ _index.json            # Index c·ªßa crawled forms
‚îî‚îÄ‚îÄ all_forms.json             # Merged (manual + crawled = 6 forms)

crawler_output/
‚îú‚îÄ‚îÄ *.pdf, *.docx              # Raw files
‚îî‚îÄ‚îÄ downloaded_files.csv       # Metadata
```

## Features

### ‚úÖ Form Processor (`src/form_processor.py`)

- OCR text extraction (PDF, DOCX, DOC, XLS, XLSX)
- AI field detection (OpenAI GPT-4o-mini) v·ªõi fallback pattern matching
- Vietnamese text normalization
- Auto generate `form_id` t·ª´ title
- Metadata preservation (source URL, OCR confidence, keywords)

**Test**:

```bash
python src/form_processor.py --file crawler_output/mau-don.pdf
```

### ‚úÖ Form Merger (`src/form_merger.py`)

- Deduplicate by title similarity (80% threshold)
- Merge aliases v√† metadata
- Prioritize manual forms
- Source tracking (manual/crawler)

**Test**:

```bash
python src/form_merger.py --threshold 0.8
```

### ‚úÖ Form Search (`src/form_search.py`)

- Vietnamese text normalization (ƒë ‚Üí d, √° ‚Üí a, etc.)
- Fuzzy matching v·ªõi relevance scoring
- Keyword indexing cho fast lookup
- Search by title, aliases, form_id

**Test**:

```bash
python src/form_search.py "ƒë∆°n xin vi·ªác"
python src/form_search.py --list --source crawler
```

## Example Output

### Processed Form

```json
{
  "form_id": "don_phan_to",
  "title": "ƒê∆†N PH·∫¢N T·ªê",
  "aliases": ["ƒë∆°n ph·∫£n t·ªë", "ph·∫£n t·ªë"],
  "source": "crawler",
  "metadata": {
    "source_url": "https://luatsubaoho.com/...",
    "ocr_confidence": 1.0,
    "ocr_keywords": ["ƒë∆°n", "s·ªë", "ng√†y"]
  },
  "fields": [
    {"name": "full_name", "label": "H·ªç v√† t√™n", "type": "string"},
    {"name": "id_number", "label": "S·ªë CCCD/CMND", "type": "string"}
  ]
}
```

### Search Results

```
Search: 'ph·∫£n t·ªë'
Found: 2 results

1. ƒê∆†N PH·∫¢N T·ªê
   Score: 0.800 | Source: crawler | ID: don_phan_to

2. Gi·∫•y x√°c nh·∫≠n c∆∞ tr√∫
   Score: 0.370 | Source: manual | ID: xac_nhan_cu_tru
```

## Workflow Integration

### Manual Workflow

```bash
# 1. Crawl forms
make crawler-run

# 2. Process + merge
make forms-pipeline

# 3. Search to verify
make forms-search Q="ƒë∆°n"
```

### Automated (GitHub Actions)

Workflow `.github/workflows/process-forms.yml` (TODO):

- Trigger sau khi Daily Crawler ho√†n th√†nh
- Auto-process forms
- Auto-merge v·ªõi manual forms
- Commit changes
- Notify qua GitHub Issues

## API Integration (TODO)

Endpoints c·∫ßn th√™m v√†o `app.py`:

```python
@app.get("/forms/search")
def search_forms(q: str, min_score: float = 0.3):
    """Search forms by query"""
    from src.form_search import FormSearch
    searcher = FormSearch()
    return searcher.search(q, min_score=min_score)

@app.get("/forms/{form_id}")
def get_form(form_id: str):
    """Get form by ID"""
    from src.form_search import FormSearch
    searcher = FormSearch()
    return searcher.search_by_id(form_id)
```

## Performance

| Task | Time | Notes |
|------|------|-------|
| OCR extraction (PDF text) | ~0.5s | PyPDF2 |
| OCR extraction (scanned) | ~3s | pdf2image + pytesseract |
| AI field detection | ~2s | OpenAI GPT-4o-mini |
| Pattern matching (fallback) | ~0.01s | Regex |
| Merge (5+1 forms) | ~0.1s | SequenceMatcher |
| Search (100 forms) | ~0.05s | Indexed lookup |

## Troubleshooting

### OpenAI API Error

```bash
# Error: Client.__init__() got an unexpected keyword argument 'proxies'
# Solution: Update openai package
pip install --upgrade openai
```

### No fields extracted

```bash
# Check OCR extraction
python -c "from src.ocr_validator import OCRValidator; \
  from pathlib import Path; \
  ocr = OCRValidator(); \
  result = ocr.validate_file(Path('file.pdf')); \
  print(result)"
```

### Duplicate not merged

```bash
# Lower threshold
python src/form_merger.py --threshold 0.7
```

## Documentation

- [Full Documentation](docs/FORM_PROCESSING.md)
- [Crawler Documentation](docs/CRAWLER_OCR.md)
- [Vietnamese Crawler](docs/VIETNAMESE_CRAWLER.md)

## Next Steps

- [ ] Add API endpoints to `app.py`
- [ ] Create GitHub Actions workflow
- [ ] Add manual review interface
- [ ] Implement Redis caching for search
- [ ] Add field validation
- [ ] Auto-generate validators

## Status

‚úÖ **Completed**:

- Form Processor with AI + fallback
- Form Merger with deduplication
- Form Search with fuzzy matching
- Documentation
- Makefile commands
- Local testing successful

‚è≥ **In Progress**:

- API endpoints integration
- GitHub Actions workflow

üìã **TODO**:

- Manual review interface
- Redis integration
- Admin UI
