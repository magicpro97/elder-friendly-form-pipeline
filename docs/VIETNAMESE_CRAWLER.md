# Vietnamese Form Crawler

## Tá»•ng quan

Crawler chuyÃªn dá»¥ng Ä‘á»ƒ thu tháº­p cÃ¡c máº«u Ä‘Æ¡n, biá»ƒu máº«u tiáº¿ng Viá»‡t tá»« cÃ¡c trang web phÃ¡p luáº­t, thá»§ tá»¥c hÃ nh chÃ­nh.

## TÃ­nh nÄƒng chÃ­nh

### ğŸ¯ Crawl 2 cáº¥p Ä‘á»™

- **Level 1**: QuÃ©t trang chÃ­nh Ä‘á»ƒ tÃ¬m links
- **Level 2**: QuÃ©t cÃ¡c trang con Ä‘á»ƒ tÃ¬m files

### ğŸ“… Lá»c theo ngÃ y Ä‘Äƒng

- Chá»‰ crawl cÃ¡c form Ä‘Äƒng sau `DB_DATE` (máº·c Ä‘á»‹nh: 2024-01-01)
- Há»— trá»£ format Viá»‡t Nam: `dd/mm/yyyy`, `yyyy-mm-dd`
- Tá»± Ä‘á»™ng loáº¡i bá» cÃ¡c trang cÅ©

### ğŸ” Lá»c theo tá»« khÃ³a

- **CRITICAL_KEYWORDS**: `máº«u`, `Ä‘Æ¡n`, `biá»ƒu máº«u`, `tá» khai`, `phiáº¿u Ä‘Äƒng kÃ½`, v.v.
- Chá»‰ download files cÃ³ chá»©a tá»« khÃ³a quan trá»ng
- TrÃ¡nh download files khÃ´ng liÃªn quan

### ğŸ“Š CSV Export

```csv
Tieu_de_trang,Link_file,Ten_file,Dang_tep,Ngay_dang
"Máº«u Ä‘Æ¡n Ä‘Äƒng kÃ½",https://...,mau-don.pdf,.pdf,2024-01-15
```

### ğŸ›¡ï¸ Anti-bot Protection

- Sá»­ dá»¥ng `cloudscraper` Ä‘á»ƒ bypass Cloudflare, anti-bot
- Automatic retry vá»›i exponential backoff
- Configurable delays giá»¯a cÃ¡c requests

## CÃ i Ä‘áº·t

### 1. Dependencies

```bash
# Trong virtual environment hoáº·c Docker
pip install -r requirements-crawler.txt
```

Hoáº·c thÃªm vÃ o Docker:

```dockerfile
RUN pip install -r requirements-crawler.txt
```

### 2. Configuration

Copy vÃ  chá»‰nh sá»­a file config:

```bash
cp .env.crawler.example .env
```

Chá»‰nh sá»­a `.env`:

```bash
# Target URLs (phÃ¢n cÃ¡ch báº±ng dáº¥u pháº©y)
CRAWLER_TARGETS=https://thuvienphapluat.vn,https://luatsubaoho.com

# Tá»« khÃ³a cáº§n tÃ¬m
CRITICAL_KEYWORDS=máº«u,Ä‘Æ¡n,biá»ƒu máº«u,tá» khai

# Chá»‰ crawl form sau ngÃ y nÃ y
DB_DATE=2024-01-01
```

## Sá»­ dá»¥ng

### Local Execution

```bash
# Cháº¡y crawler
python3 src/vietnamese_form_crawler.py

# Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u táº¡i:
# - crawler_output/downloaded_files.csv
# - crawler_output/crawler.log
# - crawler_output/*.pdf, *.doc, *.xlsx, ...
```

### GitHub Actions (Tá»± Ä‘á»™ng hÃ ng ngÃ y)

Crawler cháº¡y tá»± Ä‘á»™ng **má»—i ngÃ y lÃºc 2:00 AM UTC** (9:00 AM Viá»‡t Nam).

#### Setup Secrets

VÃ o **Settings â†’ Secrets and variables â†’ Actions**:

1. **Secrets** (bÃ­ máº­t):
   - `CRAWLER_TARGETS`: Danh sÃ¡ch URLs cáº§n crawl

2. **Variables** (cÃ´ng khai):
   - `CRITICAL_KEYWORDS`: Tá»« khÃ³a (tÃ¹y chá»n)
   - `DB_DATE`: NgÃ y cutoff (tÃ¹y chá»n, máº·c Ä‘á»‹nh: 2024-01-01)

#### Manual Trigger

1. VÃ o tab **Actions**
2. Chá»n workflow **Daily Crawler**
3. Click **Run workflow**

## Output

### File Structure

```
crawler_output/
â”œâ”€â”€ downloaded_files.csv      # Danh sÃ¡ch files Ä‘Ã£ táº£i
â”œâ”€â”€ crawler.log               # Execution logs
â”œâ”€â”€ mau-don-dang-ky.pdf       # Downloaded forms
â”œâ”€â”€ bieu-mau-to-khai.xlsx
â””â”€â”€ phieu-dang-ky.doc
```

### CSV Format

| Tieu_de_trang | Link_file | Ten_file | Dang_tep | Ngay_dang |
|---------------|-----------|----------|----------|-----------|
| Máº«u Ä‘Æ¡n xin viá»‡c | https://... | mau-don.pdf | .pdf | 2024-01-15 |

## Use Cases

### 1. Thu tháº­p máº«u Ä‘Æ¡n hÃ nh chÃ­nh

```bash
CRAWLER_TARGETS="https://thuvienphapluat.vn,https://luatsubaoho.com"
CRITICAL_KEYWORDS="máº«u,Ä‘Æ¡n,biá»ƒu máº«u"
DB_DATE=2024-01-01
```

### 2. Crawl máº«u Ä‘Æ¡n Ä‘áº¥t Ä‘ai

```bash
CRAWLER_TARGETS="https://thuviennhadat.vn"
CRITICAL_KEYWORDS="máº«u,Ä‘Æ¡n,Ä‘áº¥t Ä‘ai,biáº¿n Ä‘á»™ng"
```

### 3. Crawl máº«u hÃ³a Ä‘Æ¡n, chá»©ng tá»«

```bash
CRAWLER_TARGETS="https://thuvienphapluat.vn"
CRITICAL_KEYWORDS="hÃ³a Ä‘Æ¡n,chá»©ng tá»«,phiáº¿u"
```

## Customization

### ThÃªm tá»« khÃ³a má»›i

Chá»‰nh sá»­a `src/settings.py`:

```python
CRITICAL_KEYWORDS = [
    "máº«u", "Ä‘Æ¡n", "biá»ƒu máº«u", "tá» khai",
    "giáº¥y chá»©ng nháº­n",  # ThÃªm tá»« khÃ³a má»›i
    "giáº¥y phÃ©p",
]
```

### ThÃªm file extension

Chá»‰nh sá»­a `src/vietnamese_form_crawler.py`:

```python
FILE_EXTENSIONS = [
    ".pdf", ".doc", ".docx",
    ".ppt", ".pptx",  # ThÃªm PowerPoint
]
```

### Custom date parsing

Chá»‰nh sá»­a `DATE_PATTERNS` trong `VietnameseFormCrawler`:

```python
DATE_PATTERNS = [
    r"(\d{1,2}/\d{1,2}/\d{4})",
    r"(\d{4}-\d{1,2}-\d{1,2})",
    r"ngÃ y (\d{1,2}) thÃ¡ng (\d{1,2}) nÄƒm (\d{4})",  # Custom format
]
```

## Testing

### Unit Tests

```bash
# Run all tests
pytest tests/test_vietnamese_crawler.py -v

# Run specific test
pytest tests/test_vietnamese_crawler.py::TestVietnameseFormCrawler::test_extract_date_from_html -v

# With coverage
pytest tests/test_vietnamese_crawler.py --cov=src.vietnamese_form_crawler
```

### Manual Testing

```bash
# Test vá»›i 1 URL
export CRAWLER_TARGETS="https://thuvienphapluat.vn/hoi-dap-phap-luat/tong-hop-mau-don-xin-viec-moi-nhat-va-huong-dan-cach-viet-11482"
python3 src/vietnamese_form_crawler.py
```

## Troubleshooting

### "No files downloaded"

**NguyÃªn nhÃ¢n**:

- URLs khÃ´ng cÃ³ files má»›i sau `DB_DATE`
- Tá»« khÃ³a khÃ´ng match vá»›i ná»™i dung trang
- Website bá»‹ anti-bot block

**Giáº£i phÃ¡p**:

```bash
# Giáº£m DB_DATE Ä‘á»ƒ test
DB_DATE=2020-01-01

# ThÃªm tá»« khÃ³a
CRITICAL_KEYWORDS="máº«u,Ä‘Æ¡n,táº£i,download"

# Check logs
tail -f crawler_output/crawler.log
```

### "Cloudscraper failed"

**NguyÃªn nhÃ¢n**: Website cÃ³ Cloudflare protection máº¡nh

**Giáº£i phÃ¡p**:

```bash
# Install latest cloudscraper
pip install --upgrade cloudscraper

# Hoáº·c dÃ¹ng Selenium (cháº­m hÆ¡n)
# Chá»‰nh sá»­a code Ä‘á»ƒ dÃ¹ng Selenium
```

### "Date not found"

**NguyÃªn nhÃ¢n**: Website khÃ´ng cÃ³ ngÃ y Ä‘Äƒng hoáº·c format khÃ¡c

**Giáº£i phÃ¡p**:

- ThÃªm date pattern má»›i vÃ o `DATE_PATTERNS`
- Hoáº·c táº¯t date filtering (set `DB_DATE=2000-01-01`)

## Architecture

```
VietnameseFormCrawler
â”œâ”€â”€ __init__()           # Initialize session, CSV
â”œâ”€â”€ extract_date()       # Parse Vietnamese dates
â”œâ”€â”€ extract_form_links() # Find links with keywords
â”œâ”€â”€ download_file()      # Download & save to CSV
â””â”€â”€ crawl_all()          # Main crawl logic (2 levels)
```

### Flow Diagram

```
1. Read CRAWLER_TARGETS from env
2. For each target URL:
   â”œâ”€â”€ Level 1: Extract links + date
   â”œâ”€â”€ Filter by date (> DB_DATE)
   â”œâ”€â”€ For each link:
   â”‚   â”œâ”€â”€ If file link â†’ download
   â”‚   â””â”€â”€ If sub-page link:
   â”‚       â”œâ”€â”€ Level 2: Extract sub-links
   â”‚       â””â”€â”€ Download files from Level 2
3. Save CSV & logs
4. Print summary
```

## Performance

- **Speed**: ~1-2 pages/second (vá»›i `DELAY_BETWEEN_REQUESTS=1.0`)
- **Storage**: ~100-500 MB cho 100 forms
- **Memory**: ~50-100 MB RAM

## Security

- âœ… No credentials stored in code
- âœ… Environment variables cho sensitive data
- âœ… GitHub Secrets cho CI/CD
- âœ… Respectful crawling (delays, User-Agent)
- âš ï¸ TuÃ¢n thá»§ `robots.txt` cá»§a tá»«ng website

## Roadmap

- [ ] Playwright/Selenium support cho JavaScript-heavy sites
- [ ] OCR cho images/scanned PDFs
- [ ] Duplicate detection (checksum-based)
- [ ] Webhook notifications (Discord/Slack)
- [ ] S3/Cloud storage integration
- [ ] Form classification (ML-based)

## License

MIT License - Xem file `LICENSE` Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.
